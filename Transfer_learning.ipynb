{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsvdxazeG0dGrSBHqtMFSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayraberrones94/FCFM/blob/master/Transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning\n",
        "\n",
        "Cuando hablamos de transfer learning nos referimos al proceso de entrenamiento de un modelo en un problema relacionado con otro modelo. La manera en que funciona es cuando tomamos los pesos ya entrenados de una arquitectura de red neuronal que fue entrenada con una gran cantidad de imagenes. \n",
        "\n",
        "Estos pesos reusados pueden ayudarnos a resolver el problema, o pueden ser un punto de partida para entrenar nuestro propio modelo y adaptarlo a un nuevo problema.\n",
        "\n",
        "Por lo general nos encontramos con librerias que ya tienen incluidas aplicaciones con estas arquitecturas, en donde se entrenaron los pesos con la base de datos IMAGENET.\n",
        "\n",
        "## Como usar estos modelos\n",
        "\n",
        "Depende de los datos con los que trabajes. Por ejemplo, puedes usar los modelos tal y comoe stan entrenados, en donde la salida o el resultado es simplemente la prediccion de la imagen que se alimenta. \n",
        "\n",
        "Pero en caso de que, por ejemplo, la base de datos que se este utilizando sea muy distinta a las imagenes que se tienen en el IMAGENET,  entonces conviene modificar algunas de las capas de salida para poder ayudar en la clasificacion de nuevos objetos.\n",
        "\n",
        "Los usos pueden variar:\n",
        "\n",
        "- **Clasificador**: Utilizar el modelo pre entrenado para clasificar directamente las imagenes.\n",
        "- **Extractor de caracteristicas independiente**: Ya sea todo o parte del modelo entrenado se usa para pre procesari imagenes y extraer informacion relevante.\n",
        "- **Extractor de caracteristicas integrado**: Es lo mismo que lo anterior, pero ahora en lugar de solo utilizar los pesos, se integran a un nuevo modelo en donde las capas de la arquitectura estan congeladas.\n",
        "- **Inicializacion de pesos**: Los pesos del modelo pre entrenado se usan para inicializar el proceso de entrenamiento de la red, en lugar de iniciar de manera aleatoria.\n",
        "\n",
        "Las arquitecturas mas populares y las que se han utilizado mas son:\n",
        "\n",
        "- VGG\n",
        "- GoogleNet o Inception\n",
        "- Residual Networks o ResNet50\n",
        "\n",
        "La libreria de Keras tiene ya implementados varios modulos que permiten utilizar estos modelos pre entrenados."
      ],
      "metadata": {
        "id": "vh3EGyqPmvlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8tCsX47ms6T",
        "outputId": "3fe0385e-5c61-4ace-8f84-01c5ad30b8d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 4s 0us/step\n",
            "553476096/553467096 [==============================] - 4s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#vgg16 model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# load model\n",
        "model = VGG16()\n",
        "# summarize the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inception v3 model\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "# load model\n",
        "model = InceptionV3()\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "xRfYDpIYqCJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "# load model\n",
        "model = ResNet50()\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LIMImTbeqEzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplos para utilizar imagenes de manera individual solo para clasificacion:\n",
        "\n",
        "- [Link 1](https://pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/)\n",
        "\n"
      ],
      "metadata": {
        "id": "Aav5UBb3qEML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como implementar las redes por si solas (sin transfer learning):\n",
        "\n",
        "- [Link](https://machinelearningmastery.com/how-to-implement-major-architecture-innovations-for-convolutional-neural-networks/)\n",
        "\n",
        "Experimento con datos (no imagenes):\n",
        "\n",
        "- [Link](https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/)"
      ],
      "metadata": {
        "id": "Cil8zR_5qB5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente ejemplo fue sacado de un curso tomado para redes neuronales. \n",
        "\n",
        "\n",
        "Como primer paso se importa la libreria `glob` la cual nos ayudara a importar las imagenes por categoria, y modificarlas para poder usar el transfer learning.\n",
        "\n",
        "Como regla inicial para todas las arquitecturas, el tamano de las imagenes debe de ser 224x224."
      ],
      "metadata": {
        "id": "RwAnMufHrVBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For training set only\n",
        "import glob\n",
        "angry = glob.glob('/content/drive/My_Drive/train_logmel/angry/*.*')\n",
        "calm = glob.glob('/content/drive/My_Drive/train_logmel/calm/*.*')\n",
        "disgust = glob.glob('/content/drive/My_Drive/train_logmel/disgust/*.*')\n",
        "fearful = glob.glob('/content/drive/My_Drive/train_logmel/fearful/*.*')\n",
        "happy = glob.glob('/content/drive/My_Drive/train_logmel/happy/*.*')\n",
        "neutral = glob.glob('/content/drive/My_Drive/train_logmel/neutral/*.*')\n",
        "sad = glob.glob('/content/drive/My_Drive/train_logmel/sad/*.*')\n",
        "surprised = glob.glob('/content/drive/My_Drive/train_logmel/surprised/*.*')\n",
        "data = []\n",
        "labels = []\n",
        "for i in angry:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Angry')\n",
        "for i in calm:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Calm')\n",
        "for i in disgust:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Disgust')\n",
        "for i in fearful:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Fearful')\n",
        "for i in happy:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Happy')\n",
        "for i in neutral:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Neutral')\n",
        "for i in sad:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Sad')\n",
        "for i in surprised:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n",
        "    target_size= (224,224))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Surprised')\n",
        "train_data = np.array(data)\n",
        "train_labels = np.array(labels)"
      ],
      "metadata": {
        "id": "y3fwpes9ra2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos los datos y hacemos una transformacion a las etiquetas que pusimos anteriormente."
      ],
      "metadata": {
        "id": "WF1ycCxlrfcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "lb = LabelEncoder()\n",
        "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
      ],
      "metadata": {
        "id": "84caHXtHrdi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos desde la aplicacion de Keras los pesos que vamos a utilizar, en este caso vamos a usar la arquitectura de VGG16.\n",
        "\n",
        "Cuando corran este parte del programa les debe de dar como salida las capas entrenables. En caso de que no quieran moverle a los pesos, todas deben de estar puestas como False."
      ],
      "metadata": {
        "id": "eqUoOKzQraCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "vgg_model = VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in vgg_model.layers:\n",
        "layer.trainable = False\n",
        "# Make sure you have frozen the correct layers\n",
        "for i, layer in enumerate(vgg_model.layers):\n",
        "    print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "nIb1vxKZr6hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora tenemos las capas que podemos modificar. Recuerden que estas se agregan solo en caso de que las imagenes con las que esten trabajando sean muy diferentes de las que se encuentran en el IMAGENET.\n",
        "\n",
        "Se especifica en la ultima capa densa cuantas clases esperan tener. En este caso ponemos 8 por 8 clases distintas. "
      ],
      "metadata": {
        "id": "IXi2zTP0sSsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = vgg_model.output\n",
        "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(8, activation='softmax')(x) # Softmax for multiclass\n",
        "transfer_model = Model(inputs=vgg_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "CfRSSw89r9RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Igual que en el codigo de la tarea anterior, compilamos nuestra arquitectura. Sigan tomando en cuenta la experimentacion pasada para saber cual valor de perdida y optimizador deben de utilizar. Pueden usar el `Imagedatagenerator` que utilizamos en el codigo pasado para hacer mas grande sus datos, o pueden aplicar preprocesamiento a sus imagenes desde antes."
      ],
      "metadata": {
        "id": "HRStyDVRr-Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate= 5e-5\n",
        "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])\n",
        "history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=50, validation_data=(X_test,y_test))"
      ],
      "metadata": {
        "id": "dpHxqkbGsrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Al final podemos reciclar el mismo final de codigo que usamos para la tarea pasada para imprimir los resultados en una grafica y en la tabla de metrica de evaluacion que hayan  elegido."
      ],
      "metadata": {
        "id": "akzVafGItBta"
      }
    }
  ]
}